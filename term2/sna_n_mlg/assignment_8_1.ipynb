{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment — Link prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1. Dataset for link prediction (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider link prediction on the [e-mails network](http://snap.stanford.edu/data/email-Eu-core-temporal.html) where nodes are members of a research institution and edges are e-mails given with timestamps. The goal is to predict occurrence of edges in the test time period using information from the train time period only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://raw.githubusercontent.com/netspractice/network-science/main/datasets/email-Eu-core-temporal.txt'\n",
    "open('email-Eu-core-temporal.txt', 'wb').write(requests.get(url).content);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sender</th>\n",
       "      <th>receiver</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>582</td>\n",
       "      <td>364</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168</td>\n",
       "      <td>472</td>\n",
       "      <td>2797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>168</td>\n",
       "      <td>912</td>\n",
       "      <td>3304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>790</td>\n",
       "      <td>4523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>322</td>\n",
       "      <td>7926</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sender  receiver  timestamp\n",
       "0     582       364          0\n",
       "1     168       472       2797\n",
       "2     168       912       3304\n",
       "3       2       790       4523\n",
       "4       2       322       7926"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "email_df = pd.read_csv(\n",
    "    'email-Eu-core-temporal.txt', \n",
    "    delimiter=' ', \n",
    "    names=['sender', 'receiver', 'timestamp']\n",
    ")\n",
    "email_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, consider the following preprocessing procedure:\n",
    "1. Select edges by given train and test time periods, for example, [0, 1000) is train and [1000, 2000) is test\n",
    "2. Build a _core_ — a connected network where every edge occurs at least $k_\\text{train}$ times in the train time period or at least $k_\\text{test}$ times in the test time period. Let the core be undirected, so occurrences edges (1, 0) and (0, 1) are computed together.\n",
    "3. From the core, select a train set of edges $E_\\text{train}$ that occur for the first time in the train period. All others are included to $E_\\text{test}$.\n",
    "3. Exclude test edges that contain nodes that do not occur in train edges.\n",
    "\n",
    "Write a function `train_test_edges` that takes a pd.DataFrame `email_df` with e-mail network, a tuple with the train time period borders `train_period`, say, (0, 1000), a similar tuple `test_period`, the number of edges occurrences `ktrain` and `ktest`. The function returns two lists with tuples — train and test edges. Every edge is returned of the form where the first node is less than the second, for example [(1, 2), (2, 3)] is ok, but [(2, 1), (3, 2)] is wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d02465886cf63caec30af195db4cf5b9",
     "grade": false,
     "grade_id": "cell-676bd18fcab3c342",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_test_edges(email_df, train_period, test_period, ktrain, ktest):\n",
    "    \n",
    "    email_temp = email_df.copy()\n",
    "    email_temp = email_temp[\n",
    "        (train_period[0] <= email_temp.timestamp) \n",
    "        & (email_temp.timestamp < test_period[1])\n",
    "    ]\n",
    "    email_temp['from'] = email_temp[['sender', 'receiver']].min(axis=1)\n",
    "    email_temp['to'] = email_temp[['sender', 'receiver']].max(axis=1)\n",
    "    email_temp = email_temp.drop(['sender', 'receiver'], axis=1)\n",
    "    email_temp = email_temp.set_index(['from', 'to'])\n",
    "\n",
    "    email_train = email_temp[email_temp.timestamp < train_period[1]]\n",
    "    email_train = email_train.groupby(['from', 'to']).count()\n",
    "    train_core = email_train[email_train.timestamp >= ktrain].index.tolist()\n",
    "\n",
    "    email_test = email_temp[test_period[0] <= email_temp.timestamp]\n",
    "    email_test = email_test.groupby(['from', 'to']).count()\n",
    "    test_core = email_test[email_test.timestamp >= ktest].index.tolist()\n",
    "    \n",
    "    core = list(set(train_core + test_core))\n",
    "    \n",
    "    first_edges = email_temp.loc[core].groupby(['from', 'to']).min() \n",
    "    train_edges = first_edges[first_edges.timestamp < test_period[0]].index.tolist() \n",
    "    test_edges = first_edges[first_edges.timestamp >= test_period[0]].index.tolist()\n",
    "    train_nodes = np.unique(train_edges) \n",
    "    test_edges = [(u, v) for (u, v) in test_edges if u in train_nodes and v in train_nodes]\n",
    "    \n",
    "    return train_edges, test_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "da565e34660337c25e9701492519f95a",
     "grade": true,
     "grade_id": "cell-a77f2fa764e87595",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "train_edges, test_edges = train_test_edges(email_df, (1e7, 2e7), (2e7, 2.5e7), 3, 3)\n",
    "_train_edges, _test_edges = np.array(train_edges), np.array(test_edges)\n",
    "assert np.all(_train_edges[:, 0] < _train_edges[:, 1])\n",
    "assert np.all(_test_edges[:, 0] < _test_edges[:, 1])\n",
    "assert len(set(train_edges).intersection(test_edges)) == 0\n",
    "assert _train_edges.shape == (4147, 2)\n",
    "assert _test_edges.shape == (391, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes = sorted(set(np.array(train_edges).flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2. Negative sampling (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usually, graphs are sparse, so there is a high imbalance between positive (edge exists) and negative classes.\n",
    "To eliminate this problem, we can use the undersampling technique. \n",
    "\n",
    "The `negative_sampling` function samples the unexisted edges from the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "550f4ed4fd88b68491c86ca5c1310695",
     "grade": false,
     "grade_id": "cell-2086061e022dc394",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def negative_sampling(train_edges, test_edges):\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(np.max(train_edges + test_edges) + 1))\n",
    "    G.add_edges_from(train_edges + test_edges)\n",
    "    A = nx.to_numpy_array(G)\n",
    "    sim = (A @ A) * (1 - A)\n",
    "    sim[range(sim.shape[0]), range(sim.shape[0])] = 0\n",
    "    sorting = np.argsort(sim[sim > 0])[::-1]\n",
    "    sample = np.argwhere(sim > 0)[sorting][:len(test_edges) * 2:2]\n",
    "    return [(i, j) for i, j in sample]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1bc37a39b370339c2b5af888964df05e",
     "grade": true,
     "grade_id": "cell-e71e5d52cb2cae88",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "negatives = negative_sampling(train_edges, test_edges)\n",
    "assert len(negatives) == len(test_edges)\n",
    "assert len(set(negatives) & set(test_edges)) == 0\n",
    "assert len(set(negatives) & set(train_edges)) == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build test pairs of nodes that contains test and negative edges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 0, 0, 0, 0, 0, 1, 1, 1, 1]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pairs = np.random.permutation(negatives + test_edges)\n",
    "y_true = [int((u, v) in test_edges) for (u, v) in test_pairs]\n",
    "y_true[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3. Similarity based algorithm (0 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The similarity based algorithm:\n",
    "1. Compute similarity matrix for test pairs using a graph on train edges\n",
    "2. Order that pairs in descending of similarity\n",
    "3. Select some threshold and predict links for all pairs above the threshold\n",
    "\n",
    "Write a function `sim_link_prediction` that takes a list with train edges, test pairs and true lables. The function predicts links and returns a tuple with metrics: \n",
    "* two np.arrays: FPR (false positive rate) and TPR (true positive rate) in descending of thresholds obtained by Jaccard coefficient, `nx.jaccard_coefficient`\n",
    "* the same, by Adamic/Adar index, `nx.adamic_adar_index`\n",
    "* the same, by resource allocation index, `nx.resource_allocation_index`\n",
    "\n",
    "_Hint: use `sklearn.metrics.roc_curve`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1d3210041a30feb318d3198a746276a6",
     "grade": false,
     "grade_id": "cell-4125af6f7f2c4f56",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def sim_link_prediction(train_edges, test_pairs, y_true):\n",
    "    test_edges = [(u, v) for [u, v] in test_pairs]\n",
    "    \n",
    "    nodes = sorted(set(np.array(train_edges + test_edges).flatten()))\n",
    "    train_edges = set(train_edges)\n",
    "    test_edges = set(test_edges)\n",
    "    \n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(nodes)\n",
    "    G.add_edges_from(train_edges)\n",
    "    \n",
    "    to_compute = []\n",
    "#     y_true = []\n",
    "    for u in G.nodes:\n",
    "        for v in G.nodes:\n",
    "            if u >= v:\n",
    "                continue\n",
    "            if (u, v) in train_edges:\n",
    "                continue\n",
    "            to_compute.append((u, v))\n",
    "    \n",
    "    res = []\n",
    "    for func in (nx.jaccard_coefficient, nx.adamic_adar_index, nx.resource_allocation_index):\n",
    "        y_score = [p for u, v, p in func(G, to_compute)]\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_score)\n",
    "        res.append((fpr, tpr))\n",
    "    return tuple(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e3033a917096b4adfdfce88d37eb630e",
     "grade": true,
     "grade_id": "cell-ea34274053a92113",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [7, 241904]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-125-973f2734b108>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m jac, adam, res = sim_link_prediction(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mtrain_edges\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m537\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m120\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m267\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m630\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m37\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m730\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m251\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m887\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m773\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m97\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m327\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m506\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m280\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m687\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m )\n",
      "\u001b[0;32m<ipython-input-124-5125b3948526>\u001b[0m in \u001b[0;36msim_link_prediction\u001b[0;34m(train_edges, test_pairs, y_true)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjaccard_coefficient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madamic_adar_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_allocation_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mto_compute\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m         \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mroc_curve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m         \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0;31m# extra_args > 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36mroc_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight, drop_intermediate)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m     \"\"\"\n\u001b[0;32m--> 913\u001b[0;31m     fps, tps, thresholds = _binary_clf_curve(\n\u001b[0m\u001b[1;32m    914\u001b[0m         y_true, y_score, pos_label=pos_label, sample_weight=sample_weight)\n\u001b[1;32m    915\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36m_binary_clf_curve\u001b[0;34m(y_true, y_score, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    692\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_score\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m     \u001b[0my_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0my_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcolumn_or_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0m\u001b[1;32m    320\u001b[0m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[1;32m    321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [7, 241904]"
     ]
    }
   ],
   "source": [
    "jac, adam, res = sim_link_prediction(\n",
    "    train_edges, \n",
    "    [[537, 120], [267, 630], [37, 730], [251, 887], [773, 97], [327, 506], [280, 687]],\n",
    "    [0, 1, 1, 0, 0, 1, 0]\n",
    ")\n",
    "assert jac[0].shape == jac[1].shape\n",
    "assert adam[0].shape == adam[1].shape\n",
    "assert res[0].shape == res[1].shape\n",
    "assert round(auc(jac[0], jac[1]), 4) == 1\n",
    "assert round(auc(adam[0], adam[1]), 4) == 0.8333\n",
    "assert round(auc(res[0], res[1]), 4) == 0.9167"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8583"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(auc(jac[0], jac[1]), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at ROC AUC curve to compare similaritites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jac, adam, res = sim_link_prediction(train_edges, test_pairs, y_true)\n",
    "plt.figure(figsize=(10, 6))\n",
    "cases = [[jac[0], jac[1], 'Jaccard'], \n",
    "         [adam[0], adam[1], 'Adamic/Adar'], \n",
    "         [res[0], res[1], 'Resource alloc.']]\n",
    "for fpr, tpr, label in cases:\n",
    "    plt.plot(fpr, tpr, lw=2, \n",
    "             label='{}, AUC={:.4f}'.format(label, auc(fpr, tpr)))\n",
    "plt.plot([0, 1], [0, 1], lw=2, linestyle='--', label='Random, AUC=0.5')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4. Reconstruction adjacency matrix using SVD (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similarly to the node classification task, node embeddings could be helpful in the link prediction problem. A simple way to obtain similarity score for link prediction is to reconstruct adjacency matrix using dot product of truncated SVD node embeddings $$\\tilde A_{ij} = \\langle e_i, e_j \\rangle$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(np.max(train_edges) + 1))\n",
    "G.add_edges_from(train_edges)\n",
    "A = nx.to_numpy_array(G)\n",
    "model = TruncatedSVD(n_components=4, random_state=0)\n",
    "emb = model.fit_transform(A)\n",
    "emb.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `dot_product_scores` that takes node embeddings, test pairs, returns reconstructed scores for test pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f2eb68405b8b45411bfd09ce363857f9",
     "grade": false,
     "grade_id": "cell-e3728903f481eed8",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def dot_product_scores(emb, test_pairs):\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6109a9d205365fc16f2d6b5d1cd64cc4",
     "grade": true,
     "grade_id": "cell-d7b6b9f3c20abc71",
     "locked": true,
     "points": 3,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "scores = dot_product_scores(emb, test_pairs)\n",
    "assert scores.shape == (782, )\n",
    "fpr, tpr, _ = roc_curve(y_true, scores)\n",
    "assert auc(fpr, tpr) > 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, lw=2, \n",
    "         label='{}, AUC={:.4f}'.format('Adjacency SVD', auc(fpr, tpr)))\n",
    "plt.plot([0, 1], [0, 1], lw=2, linestyle='--', label='Random, AUC=0.5')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5. Edge emdedding (3 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task we will solve the link prediction as classification task on edge embeddings. Let us compare several techniques of edge embedding calculation from the [paper](https://peerj.com/articles/cs-172/#table-2). We will compare the different vector aggregations as features for `sklearn.linear_model.LogisticRegression` for classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.8326, -0.9151, -0.5147,  2.339 ],\n",
       "       [ 0.049 ,  0.0031, -0.0374, -0.0319],\n",
       "       [ 2.0368,  0.8496,  0.2487,  0.7585],\n",
       "       ...,\n",
       "       [ 0.0062, -0.0032, -0.0086,  0.0043],\n",
       "       [ 0.137 , -0.0427, -0.1049, -0.0812],\n",
       "       [ 0.471 , -0.4592, -0.4643,  1.4151]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G = nx.Graph()\n",
    "G.add_nodes_from(range(np.max(train_edges) + 1))\n",
    "G.add_edges_from(train_edges)\n",
    "A = nx.to_numpy_array(G)\n",
    "model = TruncatedSVD(n_components=4, n_iter=100)\n",
    "emb = model.fit_transform(A)\n",
    "emb.round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All following functions should return np.array with embeddings of edges from edges param. Average operator is simple elementwise average of node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "58f462d94128ad39168d5c1651115bbc",
     "grade": false,
     "grade_id": "cell-26173c0c479136b7",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def average_operator(G, embeddings, edges):\n",
    "    return (embeddings[edges[:, 0]] + embeddings[edges[:, 1]]) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bc61e04094cfdf04d4a4357b8afd1c8f",
     "grade": true,
     "grade_id": "cell-eaefe9963be3c2d0",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(\n",
    "    average_operator(G, emb, np.array([[42,70]])).round(2),\n",
    "    [[ 0.55, -0.14, -0.26, -0.13]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadamard product is an elementwise product of node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "dca0930cdead56210ac1d3ed38a1da2d",
     "grade": false,
     "grade_id": "cell-f662abe3d0579575",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def hadamard_operator(G, embeddings, edges):\n",
    "    return embeddings[edges[:, 0]] * embeddings[edges[:, 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "440462a8cfd997877c13c9929e7b5772",
     "grade": true,
     "grade_id": "cell-db04660af550adc7",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(\n",
    "    hadamard_operator(G, emb, np.array([[42,70]])).round(2),\n",
    "    [[0.2 , 0.02, 0.06, 0.02]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted L1 is a absolute of elementwise difference between node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d6257c4c2eb4680600acc5867ddba862",
     "grade": false,
     "grade_id": "cell-5e44fbdf6a64715d",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def weighted_l1_operator(G, embeddings, edges):\n",
    "    return abs(embeddings[edges[:, 0]] - embeddings[edges[:, 1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec862cd555da0fe6278aa191d0855aff",
     "grade": true,
     "grade_id": "cell-f2c7878af729fcb5",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(\n",
    "    weighted_l1_operator(G, emb, np.array([[42,70]])).round(2),\n",
    "    [[0.64, 0.07, 0.16, 0.11]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weighted L2 is a square of elementwise difference between node embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c81ca1cb9941e2216cb755dcb6b1d6df",
     "grade": false,
     "grade_id": "cell-0f7f5b3663337374",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def weighted_l2_operator(G, embeddings, edges):\n",
    "    return (embeddings[edges[:, 0]] - embeddings[edges[:, 1]]) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1e6fb9bbeff70f61d8b1c3974464baee",
     "grade": true,
     "grade_id": "cell-957e1faea3e9127d",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(\n",
    "    weighted_l2_operator(G, emb, np.array([[42,70]])).round(2),\n",
    "    [[0.41, 0.01, 0.03, 0.01]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighbor weighted L1 is a absolute of elementwise difference between mean embeddings of node neigbors with itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91862e3967ecf48894e45a0adf8af354",
     "grade": false,
     "grade_id": "cell-9663ec4df9dbb751",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def neighbor_weighted_l1_operator(G, embeddings, edges):\n",
    "    res = []\n",
    "    for edge in edges:\n",
    "        u, v = edge\n",
    "        a = [embeddings[x] for x in G.neighbors(u)]\n",
    "        a.append(embeddings[u])\n",
    "        a = np.array(a)\n",
    "        b = [embeddings[x] for x in G.neighbors(v)]\n",
    "        b.append(embeddings[v])\n",
    "        b = np.array(b)\n",
    "        score = abs(a.mean(axis=0) - b.mean(axis=0))\n",
    "        res.append(score)\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1cbefe98af2a1b6b2376f799b866d939",
     "grade": true,
     "grade_id": "cell-fe7f124fa265373c",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(\n",
    "    neighbor_weighted_l1_operator(G, emb, np.array([[42,70]])).round(2),\n",
    "    [[2.17, 0.18, 0.34, 0.2 ]]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Neighbor weighted L1 is a square of elementwise difference between mean embeddings of node neigbors with itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6fe87475999a79b39066260f060f80ca",
     "grade": false,
     "grade_id": "cell-cb55782d381636cd",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def neighbor_weighted_l2_operator(G, embeddings, edges):\n",
    "    return neighbor_weighted_l1_operator(G, embeddings, edges) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ae389acc9d3af30a4e104f746d1f56f",
     "grade": true,
     "grade_id": "cell-b3ea740a7d8e7422",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "assert np.allclose(\n",
    "    neighbor_weighted_l2_operator(G, emb, np.array([[42,70]])).round(2),\n",
    "    [[4.69, 0.03, 0.12, 0.04]]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-61-f9cc28d9a4fd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pairs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moperators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_pairs' is not defined"
     ]
    }
   ],
   "source": [
    "operators = {\n",
    "    \"average_operator\": average_operator,\n",
    "    \"hadamard_operator\": hadamard_operator,\n",
    "    \"weighted_l1_operator\": weighted_l1_operator,\n",
    "    \"weighted_l2_operator\": weighted_l2_operator,\n",
    "    \"neighbor_weighted_l1_operator\": neighbor_weighted_l1_operator,\n",
    "    \"neighbor_weighted_l2_operator\": neighbor_weighted_l2_operator\n",
    "}\n",
    "\n",
    "train_split = int(len(test_pairs) * 0.8)\n",
    "res = {}\n",
    "for name, operator in operators.items():\n",
    "    lr = LogisticRegression()\n",
    "    edge_emb = operator(G, emb, test_pairs)\n",
    "    lr.fit(edge_emb[:train_split], y_true[:train_split])\n",
    "    preds = lr.predict_proba(edge_emb[train_split:])[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_true[train_split:], preds)\n",
    "    res[name] = {\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for label, v in res.items():\n",
    "    fpr, tpr = v['fpr'], v['tpr']\n",
    "    plt.plot(fpr, tpr, lw=2, \n",
    "             label='{}, AUC={:.4f}'.format(label, auc(fpr, tpr)))\n",
    "plt.plot([0, 1], [0, 1], lw=2, linestyle='--', label='Random, AUC=0.5')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6. Walklets model (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gensim==4.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Walklets (Perozzi, Kulkarni & Skiena, 2016) use a weighted combination of embeddings of powers of adjacency matrix $A$, $A^2$, …, $A^k$ to reduce the bias of Deepwalk for low-order proximities, and approximates computing $A^i$ by skipping nodes using short random walks (Perozzi et al., 2017). Walklets captures multiple scales of relationships between vertices in a graph. The method itself is scalable, and can be run on graphs with millions of vertices.\n",
    "\n",
    "Firstly, we need to sample some random walks. You can take this function from the DeepWalk task of the node classification assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "999ee0e8c86557f2448d8a05ed3037b7",
     "grade": false,
     "grade_id": "cell-15f0d69f9b2d6d42",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def random_walks(G, n_walks, path_length):\n",
    "    walks = []\n",
    "    for node in G.nodes:\n",
    "        prev_node = node\n",
    "        for n_walk in range(n_walks):\n",
    "            path = [node]\n",
    "            for _ in range(path_length - 1):\n",
    "                cur_node = np.random.choice(list(G.neighbors(prev_node)))\n",
    "                path.append(cur_node)\n",
    "                prev_node = cur_node\n",
    "            walks.append(path)\n",
    "    return np.array(walks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4ed1cd07b868701427efb505f8729593",
     "grade": true,
     "grade_id": "cell-1832ce04312cc4a9",
     "locked": true,
     "points": 0.8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "walks = random_walks(nx.karate_club_graph(), 10, 5)\n",
    "\n",
    "assert walks.shape == (34*10, 5)\n",
    "for i, j in zip(walks[0, :-1], walks[0, 1:]):\n",
    "    assert nx.karate_club_graph().has_edge(i, j)\n",
    "assert np.all(walks[:, 0] == np.repeat(np.arange(34), 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to DeepWalk, we model the network through a series of truncated random walks started at each node. However, we make a key change to the sampling procedure. Specifically, we choose to skip some of the the nodes in the random walk. In this way, we form a set of relationships which are sampled from successively higher powers of A.\n",
    "\n",
    "Function `skip_steps` separates a random walk `walk` on the several walks with `n_steps` steps between nodes. It returns list of lists with random walks with skips steps, look at asserts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "27ca9a612f54a5c3a87db00e84c97339",
     "grade": false,
     "grade_id": "cell-94ff0150d47246f3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def skip_steps(walk, n_steps):\n",
    "    res = []\n",
    "    for i in range(n_steps + 1):\n",
    "        res.append(walk[i:len(walk):n_steps])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2bb15f7e8e35cb4d34d398d34f820814",
     "grade": true,
     "grade_id": "cell-cf0f3db3a583d5c7",
     "locked": true,
     "points": 0.8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "walk = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "assert skip_steps(walk, 2) == [[0, 2, 4, 6, 8], [1, 3, 5, 7, 9], [2, 4, 6, 8]]\n",
    "assert skip_steps(walk, 3) == [[0, 3, 6, 9], [1, 4, 7], [2, 5, 8], [3, 6, 9]]\n",
    "\n",
    "skipped = skip_steps(walks[0], 2)\n",
    "assert len(skipped) == 3\n",
    "assert len(skipped[1]) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we prepare walklets as follows: skip steps in each random walk and union them into a single list. Write a function `generate_walklets` that takes input random walks and number of steps, return a dataset with skipped random walks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "50b5c40024da6b6fafc6f52cc67f2405",
     "grade": false,
     "grade_id": "cell-37b67a0e80a3f3d3",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_walklets(input_walks, n_steps):\n",
    "    input_walks = [list(w) for w in input_walks]\n",
    "    res = []\n",
    "    for walk in input_walks:\n",
    "        for walk_skips in skip_steps(walk, n_steps):\n",
    "            res.append(walk_skips)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30dfa61724fb749e61147204b78e1b95",
     "grade": true,
     "grade_id": "cell-92b5dc686df237f5",
     "locked": true,
     "points": 0.8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "example_walks = [[0, 2, 4, 6, 8], [1, 3, 5, 7, 9], [2, 4, 6, 8]]\n",
    "assert (\n",
    "    generate_walklets(example_walks, 2) == \n",
    "    [[0, 4, 8], [2, 6], [4, 8], [1, 5, 9], [3, 7], [5, 9], [2, 6], [4, 8], [6]]\n",
    ")\n",
    "\n",
    "walklets = generate_walklets(walks, 2)\n",
    "assert len(walklets) == 1020\n",
    "assert len(walklets[1]) == 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train embedding you need to know the set of nodes, sampled random walks without skips, size of the maximal desired skip (window_size) and dimension of embedding for the one skip.\n",
    "\n",
    "The function `train_embedding` should work as follows:\n",
    "For each skip_length between `1` and `window_size + 1`\n",
    "1. Create dataset with splits\n",
    "2. Train Word2Vec model on the created walklets with given vector_size, min_count=1, epochs=10 and window=1.\n",
    "3. Save embeddings for the given step\n",
    "\n",
    "After all iterations you need to take a mean of received embeddings for a node from each step. Finally, we return np.array with embeddings ordered by the id of node, if node id has no embedding, then use `np.zeros(vector_size)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71fffccd20964f4663aa4d08567059c3",
     "grade": false,
     "grade_id": "cell-2dd995c4351416bf",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def train_embedding(nodes, walks, window_size=5, vector_size=8):\n",
    "    d = {node: [] for node in nodes}\n",
    "    for skip_length in range(1, window_size + 1):\n",
    "        sentences = generate_walklets(walks, skip_length)\n",
    "        wv_embeddings = Word2Vec(sentences, min_count=1, sg=1, window=1, vector_size=8).wv\n",
    "        for node in d:\n",
    "            if node in wv_embeddings:\n",
    "                d[node].append(wv_embeddings[node])\n",
    "            else:\n",
    "                d[node].append(np.zeros(vector_size))\n",
    "    embeddings = np.array([np.array(v).mean(axis=0) for v in d.values()])\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "302b7fb73d64e62624e2d87c1c39a275",
     "grade": true,
     "grade_id": "cell-caa5c52f4660af25",
     "locked": true,
     "points": 0.8,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'vector_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-7886f3ef9848>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mnodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_edges\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mwalks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom_walks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_walks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0memb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwalks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0memb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1005\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-87-4d281d3657d8>\u001b[0m in \u001b[0;36mtrain_embedding\u001b[0;34m(nodes, walks, window_size, vector_size)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mskip_length\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_size\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_walklets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwalks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mwv_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mWord2Vec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_count\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msg\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mwv_embeddings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'vector_size'"
     ]
    }
   ],
   "source": [
    "np.random.seed(0)\n",
    "G = nx.Graph(train_edges)\n",
    "nodes = np.arange(np.max(train_edges) + 1)\n",
    "walks = random_walks(G, n_walks=5, path_length=10)\n",
    "emb = train_embedding(nodes, walks, window_size=3, vector_size=8)\n",
    "assert emb.shape == (1005, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2e8dc6e7dfe8f09ed4a7d9e5b707aa33",
     "grade": true,
     "grade_id": "cell-98cf8c96d4d4ad52",
     "locked": true,
     "points": 0.7999999999999998,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_pairs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-6099d9227614>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m }\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mtrain_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_pairs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moperators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_pairs' is not defined"
     ]
    }
   ],
   "source": [
    "operators = {\n",
    "    \"average_operator\": average_operator,\n",
    "    \"hadamard_operator\": hadamard_operator,\n",
    "    \"weighted_l1_operator\": weighted_l1_operator,\n",
    "    \"weighted_l2_operator\": weighted_l2_operator,\n",
    "    \"neighbor_weighted_l1_operator\": neighbor_weighted_l1_operator,\n",
    "    \"neighbor_weighted_l2_operator\": neighbor_weighted_l2_operator\n",
    "}\n",
    "\n",
    "train_split = int(len(test_pairs) * 0.8)\n",
    "res = {}\n",
    "for name, operator in operators.items():\n",
    "    lr = LogisticRegression()\n",
    "    edge_emb = operator(G, emb, test_pairs)\n",
    "    lr.fit(edge_emb[:train_split], y_true[:train_split])\n",
    "    preds = lr.predict_proba(edge_emb[train_split:])[:, 1]\n",
    "    fpr, tpr, _ = roc_curve(y_true[train_split:], preds)\n",
    "    res[name] = {\n",
    "        'fpr': fpr,\n",
    "        'tpr': tpr\n",
    "    }\n",
    "assert auc(fpr, tpr) > 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-e15f14b2de23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mfpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'fpr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tpr'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     plt.plot(fpr, tpr, lw=2, \n\u001b[1;32m      5\u001b[0m              label='{}, AUC={:.4f}'.format(label, auc(fpr, tpr)))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'res' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "for label, v in res.items():\n",
    "    fpr, tpr = v['fpr'], v['tpr']\n",
    "    plt.plot(fpr, tpr, lw=2, \n",
    "             label='{}, AUC={:.4f}'.format(label, auc(fpr, tpr)))\n",
    "plt.plot([0, 1], [0, 1], lw=2, linestyle='--', label='Random, AUC=0.5')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC AUC')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
