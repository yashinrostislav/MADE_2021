{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86e0de040aac317a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Lab assignment â„–1, part 2\n",
    "\n",
    "This lab assignment consists of several parts. You are supposed to make some transformations, train some models, estimate the quality of the models and explain your results.\n",
    "\n",
    "Several comments:\n",
    "* Don't hesitate to ask questions, it's a good practice.\n",
    "* No private/public sharing, please. The copied assignments will be graded with 0 points.\n",
    "* Blocks of this lab will be graded separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*This is the second part of the assignment. First and third parts are waiting for you in the same directory.*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-512ba712fc0fc065",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Part 2. Data preprocessing, model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b656a4266174b009",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### 1. Reading the data\n",
    "Today we work with the [dataset](https://archive.ics.uci.edu/ml/datasets/Statlog+%28Vehicle+Silhouettes%29), describing different cars for multiclass ($k=4$) classification problem. The data is available below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If on colab, uncomment the following lines\n",
    "\n",
    "# ! wget https://raw.githubusercontent.com/girafe-ai/ml-mipt/basic_s21/homeworks_basic/Lab1_ML_pipeline_and_SVM/car_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eebac6bfdf73d0bc",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(846, 19) (846,)\n",
      "(549, 19) (549,) (297, 19) (297,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_csv('car_data.csv', delimiter=',', header=None).values\n",
    "data = dataset[:, :-1].astype(int)\n",
    "target = dataset[:, -1]\n",
    "\n",
    "print(data.shape, target.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.35)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-88b1a0f688568f2c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "To get some insights about the dataset, `pandas` might be used. The `train` part is transformed to `pd.DataFrame` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>713</td>\n",
       "      <td>107</td>\n",
       "      <td>53</td>\n",
       "      <td>108</td>\n",
       "      <td>211</td>\n",
       "      <td>63</td>\n",
       "      <td>11</td>\n",
       "      <td>219</td>\n",
       "      <td>31</td>\n",
       "      <td>25</td>\n",
       "      <td>168</td>\n",
       "      <td>228</td>\n",
       "      <td>704</td>\n",
       "      <td>198</td>\n",
       "      <td>69</td>\n",
       "      <td>10</td>\n",
       "      <td>21</td>\n",
       "      <td>190</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>230</td>\n",
       "      <td>85</td>\n",
       "      <td>35</td>\n",
       "      <td>47</td>\n",
       "      <td>110</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>117</td>\n",
       "      <td>57</td>\n",
       "      <td>17</td>\n",
       "      <td>122</td>\n",
       "      <td>136</td>\n",
       "      <td>203</td>\n",
       "      <td>139</td>\n",
       "      <td>89</td>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>180</td>\n",
       "      <td>184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>438</td>\n",
       "      <td>91</td>\n",
       "      <td>49</td>\n",
       "      <td>86</td>\n",
       "      <td>195</td>\n",
       "      <td>63</td>\n",
       "      <td>8</td>\n",
       "      <td>177</td>\n",
       "      <td>37</td>\n",
       "      <td>21</td>\n",
       "      <td>156</td>\n",
       "      <td>203</td>\n",
       "      <td>473</td>\n",
       "      <td>201</td>\n",
       "      <td>67</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>192</td>\n",
       "      <td>198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>93</td>\n",
       "      <td>44</td>\n",
       "      <td>98</td>\n",
       "      <td>197</td>\n",
       "      <td>62</td>\n",
       "      <td>11</td>\n",
       "      <td>183</td>\n",
       "      <td>36</td>\n",
       "      <td>22</td>\n",
       "      <td>146</td>\n",
       "      <td>202</td>\n",
       "      <td>505</td>\n",
       "      <td>152</td>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>195</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>440</td>\n",
       "      <td>101</td>\n",
       "      <td>51</td>\n",
       "      <td>98</td>\n",
       "      <td>194</td>\n",
       "      <td>60</td>\n",
       "      <td>10</td>\n",
       "      <td>195</td>\n",
       "      <td>34</td>\n",
       "      <td>22</td>\n",
       "      <td>161</td>\n",
       "      <td>219</td>\n",
       "      <td>572</td>\n",
       "      <td>219</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>192</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>104</td>\n",
       "      <td>102</td>\n",
       "      <td>45</td>\n",
       "      <td>83</td>\n",
       "      <td>198</td>\n",
       "      <td>65</td>\n",
       "      <td>5</td>\n",
       "      <td>194</td>\n",
       "      <td>33</td>\n",
       "      <td>22</td>\n",
       "      <td>146</td>\n",
       "      <td>225</td>\n",
       "      <td>576</td>\n",
       "      <td>167</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>27</td>\n",
       "      <td>193</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>603</td>\n",
       "      <td>90</td>\n",
       "      <td>36</td>\n",
       "      <td>85</td>\n",
       "      <td>184</td>\n",
       "      <td>64</td>\n",
       "      <td>6</td>\n",
       "      <td>160</td>\n",
       "      <td>41</td>\n",
       "      <td>20</td>\n",
       "      <td>125</td>\n",
       "      <td>187</td>\n",
       "      <td>385</td>\n",
       "      <td>139</td>\n",
       "      <td>66</td>\n",
       "      <td>9</td>\n",
       "      <td>31</td>\n",
       "      <td>195</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>555</td>\n",
       "      <td>87</td>\n",
       "      <td>49</td>\n",
       "      <td>86</td>\n",
       "      <td>190</td>\n",
       "      <td>64</td>\n",
       "      <td>9</td>\n",
       "      <td>177</td>\n",
       "      <td>37</td>\n",
       "      <td>21</td>\n",
       "      <td>153</td>\n",
       "      <td>197</td>\n",
       "      <td>471</td>\n",
       "      <td>209</td>\n",
       "      <td>67</td>\n",
       "      <td>11</td>\n",
       "      <td>7</td>\n",
       "      <td>192</td>\n",
       "      <td>199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>244</td>\n",
       "      <td>85</td>\n",
       "      <td>47</td>\n",
       "      <td>75</td>\n",
       "      <td>121</td>\n",
       "      <td>53</td>\n",
       "      <td>9</td>\n",
       "      <td>157</td>\n",
       "      <td>44</td>\n",
       "      <td>20</td>\n",
       "      <td>165</td>\n",
       "      <td>168</td>\n",
       "      <td>358</td>\n",
       "      <td>176</td>\n",
       "      <td>77</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>182</td>\n",
       "      <td>191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>256</td>\n",
       "      <td>91</td>\n",
       "      <td>36</td>\n",
       "      <td>77</td>\n",
       "      <td>157</td>\n",
       "      <td>56</td>\n",
       "      <td>7</td>\n",
       "      <td>155</td>\n",
       "      <td>42</td>\n",
       "      <td>19</td>\n",
       "      <td>126</td>\n",
       "      <td>177</td>\n",
       "      <td>361</td>\n",
       "      <td>123</td>\n",
       "      <td>65</td>\n",
       "      <td>8</td>\n",
       "      <td>15</td>\n",
       "      <td>195</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>642</td>\n",
       "      <td>95</td>\n",
       "      <td>46</td>\n",
       "      <td>104</td>\n",
       "      <td>208</td>\n",
       "      <td>66</td>\n",
       "      <td>9</td>\n",
       "      <td>191</td>\n",
       "      <td>35</td>\n",
       "      <td>22</td>\n",
       "      <td>148</td>\n",
       "      <td>210</td>\n",
       "      <td>543</td>\n",
       "      <td>169</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>28</td>\n",
       "      <td>190</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>156</td>\n",
       "      <td>85</td>\n",
       "      <td>36</td>\n",
       "      <td>78</td>\n",
       "      <td>149</td>\n",
       "      <td>55</td>\n",
       "      <td>7</td>\n",
       "      <td>147</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>128</td>\n",
       "      <td>168</td>\n",
       "      <td>321</td>\n",
       "      <td>134</td>\n",
       "      <td>64</td>\n",
       "      <td>10</td>\n",
       "      <td>24</td>\n",
       "      <td>197</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>368</td>\n",
       "      <td>84</td>\n",
       "      <td>45</td>\n",
       "      <td>68</td>\n",
       "      <td>148</td>\n",
       "      <td>64</td>\n",
       "      <td>6</td>\n",
       "      <td>146</td>\n",
       "      <td>46</td>\n",
       "      <td>19</td>\n",
       "      <td>142</td>\n",
       "      <td>168</td>\n",
       "      <td>317</td>\n",
       "      <td>180</td>\n",
       "      <td>75</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>183</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>132</td>\n",
       "      <td>97</td>\n",
       "      <td>42</td>\n",
       "      <td>101</td>\n",
       "      <td>186</td>\n",
       "      <td>59</td>\n",
       "      <td>9</td>\n",
       "      <td>186</td>\n",
       "      <td>36</td>\n",
       "      <td>22</td>\n",
       "      <td>138</td>\n",
       "      <td>208</td>\n",
       "      <td>511</td>\n",
       "      <td>168</td>\n",
       "      <td>67</td>\n",
       "      <td>7</td>\n",
       "      <td>41</td>\n",
       "      <td>194</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>10</td>\n",
       "      <td>86</td>\n",
       "      <td>36</td>\n",
       "      <td>70</td>\n",
       "      <td>143</td>\n",
       "      <td>61</td>\n",
       "      <td>9</td>\n",
       "      <td>133</td>\n",
       "      <td>50</td>\n",
       "      <td>18</td>\n",
       "      <td>130</td>\n",
       "      <td>153</td>\n",
       "      <td>266</td>\n",
       "      <td>127</td>\n",
       "      <td>66</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>194</td>\n",
       "      <td>202</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1   2    3    4   5   6    7   8   9    10   11   12   13  14  15  \\\n",
       "0   713  107  53  108  211  63  11  219  31  25  168  228  704  198  69  10   \n",
       "1   230   85  35   47  110  55   3  117  57  17  122  136  203  139  89   5   \n",
       "2   438   91  49   86  195  63   8  177  37  21  156  203  473  201  67   7   \n",
       "3     9   93  44   98  197  62  11  183  36  22  146  202  505  152  64   4   \n",
       "4   440  101  51   98  194  60  10  195  34  22  161  219  572  219  67   0   \n",
       "5   104  102  45   83  198  65   5  194  33  22  146  225  576  167  79   0   \n",
       "6   603   90  36   85  184  64   6  160  41  20  125  187  385  139  66   9   \n",
       "7   555   87  49   86  190  64   9  177  37  21  153  197  471  209  67  11   \n",
       "8   244   85  47   75  121  53   9  157  44  20  165  168  358  176  77   1   \n",
       "9   256   91  36   77  157  56   7  155  42  19  126  177  361  123  65   8   \n",
       "10  642   95  46  104  208  66   9  191  35  22  148  210  543  169  68   0   \n",
       "11  156   85  36   78  149  55   7  147  45  19  128  168  321  134  64  10   \n",
       "12  368   84  45   68  148  64   6  146  46  19  142  168  317  180  75   5   \n",
       "13  132   97  42  101  186  59   9  186  36  22  138  208  511  168  67   7   \n",
       "14   10   86  36   70  143  61   9  133  50  18  130  153  266  127  66   2   \n",
       "\n",
       "    16   17   18  \n",
       "0   21  190  203  \n",
       "1    9  180  184  \n",
       "2    5  192  198  \n",
       "3   14  195  204  \n",
       "4   10  192  201  \n",
       "5   27  193  191  \n",
       "6   31  195  203  \n",
       "7    7  192  199  \n",
       "8    7  182  191  \n",
       "9   15  195  201  \n",
       "10  28  190  200  \n",
       "11  24  197  203  \n",
       "12   1  183  187  \n",
       "13  41  194  206  \n",
       "14  10  194  202  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pd = pd.DataFrame(X_train)\n",
    "\n",
    "# First 15 rows of our dataset.\n",
    "X_train_pd.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-98e7d91d77d65fcf",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Methods `describe` and `info` deliver some useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>414.249545</td>\n",
       "      <td>93.590164</td>\n",
       "      <td>44.746812</td>\n",
       "      <td>82.056466</td>\n",
       "      <td>169.938069</td>\n",
       "      <td>61.959927</td>\n",
       "      <td>8.555556</td>\n",
       "      <td>168.679417</td>\n",
       "      <td>40.923497</td>\n",
       "      <td>20.568306</td>\n",
       "      <td>147.642987</td>\n",
       "      <td>188.579235</td>\n",
       "      <td>438.865209</td>\n",
       "      <td>174.054645</td>\n",
       "      <td>72.160291</td>\n",
       "      <td>6.369763</td>\n",
       "      <td>12.870674</td>\n",
       "      <td>189.187614</td>\n",
       "      <td>195.945355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>243.809362</td>\n",
       "      <td>8.133555</td>\n",
       "      <td>6.077932</td>\n",
       "      <td>15.804650</td>\n",
       "      <td>33.495503</td>\n",
       "      <td>8.066684</td>\n",
       "      <td>4.648514</td>\n",
       "      <td>32.802372</td>\n",
       "      <td>7.748886</td>\n",
       "      <td>2.542589</td>\n",
       "      <td>14.370617</td>\n",
       "      <td>31.048637</td>\n",
       "      <td>173.667408</td>\n",
       "      <td>31.965834</td>\n",
       "      <td>7.595126</td>\n",
       "      <td>4.965564</td>\n",
       "      <td>9.161947</td>\n",
       "      <td>6.141530</td>\n",
       "      <td>7.334595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>76.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>104.000000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>114.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>131.000000</td>\n",
       "      <td>191.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>181.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>203.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>144.000000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>147.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>168.000000</td>\n",
       "      <td>319.000000</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>185.000000</td>\n",
       "      <td>191.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>408.000000</td>\n",
       "      <td>93.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>169.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>157.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>179.000000</td>\n",
       "      <td>365.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>71.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>189.000000</td>\n",
       "      <td>197.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>627.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>98.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>196.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>159.000000</td>\n",
       "      <td>216.000000</td>\n",
       "      <td>575.000000</td>\n",
       "      <td>197.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>201.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>844.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>333.000000</td>\n",
       "      <td>138.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>262.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>186.000000</td>\n",
       "      <td>320.000000</td>\n",
       "      <td>987.000000</td>\n",
       "      <td>268.000000</td>\n",
       "      <td>135.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>204.000000</td>\n",
       "      <td>211.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  549.000000  549.000000  549.000000  549.000000  549.000000  549.000000   \n",
       "mean   414.249545   93.590164   44.746812   82.056466  169.938069   61.959927   \n",
       "std    243.809362    8.133555    6.077932   15.804650   33.495503    8.066684   \n",
       "min      1.000000   76.000000   33.000000   42.000000  104.000000   47.000000   \n",
       "25%    203.000000   87.000000   40.000000   70.000000  144.000000   57.000000   \n",
       "50%    408.000000   93.000000   44.000000   80.000000  169.000000   61.000000   \n",
       "75%    627.000000   99.000000   49.000000   98.000000  197.000000   65.000000   \n",
       "max    844.000000  119.000000   59.000000  112.000000  333.000000  138.000000   \n",
       "\n",
       "               6           7           8           9           10          11  \\\n",
       "count  549.000000  549.000000  549.000000  549.000000  549.000000  549.000000   \n",
       "mean     8.555556  168.679417   40.923497   20.568306  147.642987  188.579235   \n",
       "std      4.648514   32.802372    7.748886    2.542589   14.370617   31.048637   \n",
       "min      2.000000  114.000000   26.000000   17.000000  118.000000  131.000000   \n",
       "25%      7.000000  147.000000   34.000000   19.000000  137.000000  168.000000   \n",
       "50%      8.000000  157.000000   43.000000   20.000000  146.000000  179.000000   \n",
       "75%     10.000000  196.000000   46.000000   23.000000  159.000000  216.000000   \n",
       "max     55.000000  262.000000   59.000000   28.000000  186.000000  320.000000   \n",
       "\n",
       "               12          13          14          15          16          17  \\\n",
       "count  549.000000  549.000000  549.000000  549.000000  549.000000  549.000000   \n",
       "mean   438.865209  174.054645   72.160291    6.369763   12.870674  189.187614   \n",
       "std    173.667408   31.965834    7.595126    4.965564    9.161947    6.141530   \n",
       "min    191.000000  109.000000   60.000000    0.000000    0.000000  176.000000   \n",
       "25%    319.000000  148.000000   67.000000    2.000000    6.000000  185.000000   \n",
       "50%    365.000000  174.000000   71.000000    6.000000   11.000000  189.000000   \n",
       "75%    575.000000  197.000000   75.000000    9.000000   19.000000  194.000000   \n",
       "max    987.000000  268.000000  135.000000   22.000000   41.000000  204.000000   \n",
       "\n",
       "               18  \n",
       "count  549.000000  \n",
       "mean   195.945355  \n",
       "std      7.334595  \n",
       "min    181.000000  \n",
       "25%    191.000000  \n",
       "50%    197.000000  \n",
       "75%    201.000000  \n",
       "max    211.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 549 entries, 0 to 548\n",
      "Data columns (total 19 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   0       549 non-null    int64\n",
      " 1   1       549 non-null    int64\n",
      " 2   2       549 non-null    int64\n",
      " 3   3       549 non-null    int64\n",
      " 4   4       549 non-null    int64\n",
      " 5   5       549 non-null    int64\n",
      " 6   6       549 non-null    int64\n",
      " 7   7       549 non-null    int64\n",
      " 8   8       549 non-null    int64\n",
      " 9   9       549 non-null    int64\n",
      " 10  10      549 non-null    int64\n",
      " 11  11      549 non-null    int64\n",
      " 12  12      549 non-null    int64\n",
      " 13  13      549 non-null    int64\n",
      " 14  14      549 non-null    int64\n",
      " 15  15      549 non-null    int64\n",
      " 16  16      549 non-null    int64\n",
      " 17  17      549 non-null    int64\n",
      " 18  18      549 non-null    int64\n",
      "dtypes: int64(19)\n",
      "memory usage: 81.6 KB\n"
     ]
    }
   ],
   "source": [
    "X_train_pd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-be844269be69c387",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### 2. Machine Learning pipeline\n",
    "Here you are supposed to perform the desired transformations. Please, explain your results briefly after each task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0. Data preprocessing\n",
    "* Make some transformations of the dataset (if necessary). Briefly explain the transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a1514aa189a49fca",
     "locked": false,
     "points": 15,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Basic logistic regression\n",
    "* Find optimal hyperparameters for logistic regression with cross-validation on the `train` data (small grid/random search is enough, no need to find the *best* parameters).\n",
    "\n",
    "* Estimate the model quality with `f1` and `accuracy` scores.\n",
    "* Plot a ROC-curve for the trained model. For the multiclass case you might use `scikitplot` library (e.g. `scikitplot.metrics.plot_roc(test_labels, predicted_proba)`).\n",
    "\n",
    "*Note: please, use the following hyperparameters for logistic regression: `multi_class='multinomial'`, `solver='saga'` `tol=1e-3` and ` max_iter=500`.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1dd5ad5d0845cbbb",
     "locked": false,
     "points": 5,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might use this command to install scikit-plot. \n",
    "# Warning, if you a running locally, don't call pip from within jupyter, call it from terminal in the corresponding \n",
    "# virtual environment instead\n",
    "\n",
    "# ! pip install scikit-plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. PCA: explained variance plot\n",
    "* Apply the PCA to the train part of the data. Build the explaided variance plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c6c614740bce090e",
     "locked": false,
     "points": 10,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c1fe666f52fe53c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### 2.3. PCA trasformation\n",
    "* Select the appropriate number of components. Briefly explain your choice. Should you normalize the data?\n",
    "\n",
    "*Use `fit` and `transform` methods to transform the `train` and `test` parts.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-96ab18d96473ef71",
     "locked": false,
     "points": 5,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: From this point `sklearn` [Pipeline](https://scikit-learn.org/stable/modules/compose.html) might be useful to perform transformations on the data. Refer to the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) for more information.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d28b58a35c94e988",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### 2.4. Logistic regression on PCA-preprocessed data.\n",
    "* Find optimal hyperparameters for logistic regression with cross-validation on the transformed by PCA `train` data.\n",
    "\n",
    "* Estimate the model quality with `f1` and `accuracy` scores.\n",
    "* Plot a ROC-curve for the trained model. For the multiclass case you might use `scikitplot` library (e.g. `scikitplot.metrics.plot_roc(test_labels, predicted_proba)`).\n",
    "\n",
    "*Note: please, use the following hyperparameters for logistic regression: `multi_class='multinomial'`, `solver='saga'` and `tol=1e-3`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-12d53ea45258fa82",
     "locked": false,
     "points": 5,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4fbf16c64076e139",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### 2.5. Decision tree\n",
    "* Now train a desicion tree on the same data. Find optimal tree depth (`max_depth`) using cross-validation.\n",
    "\n",
    "* Measure the model quality using the same metrics you used above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-748ed20b51c67fab",
     "locked": false,
     "points": 15,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9eadd4d8a03ae67a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### 2.6. Bagging.\n",
    "Here starts the ensembling part.\n",
    "\n",
    "First we will use the __Bagging__ approach. Build an ensemble of $N$ algorithms varying N from $N_{min}=2$ to $N_{max}=100$ (with step 5).\n",
    "\n",
    "We will build two ensembles: of logistic regressions and of decision trees.\n",
    "\n",
    "*Comment: each ensemble should be constructed from models of the same family, so logistic regressions should not be mixed up with decision trees.*\n",
    "\n",
    "\n",
    "*Hint 1: To build a __Bagging__ ensebmle varying the ensemble size efficiently you might generate $N_{max}$ subsets of `train` data (of the same size as the original dataset) using bootstrap procedure once. Then you train a new instance of logistic regression/decision tree with optimal hyperparameters you estimated before on each subset (so you train it from scratch). Finally, to get an ensemble of $N$ models you average the $N$ out of $N_{max}$ models predictions.*\n",
    "\n",
    "*Hint 2: sklearn might help you with this taks. Some appropriate function/class might be out there.*\n",
    "\n",
    "* Plot `f1` and `accuracy` scores plots w.r.t. the size of the ensemble.\n",
    "\n",
    "* Briefly analyse the plot. What is the optimal number of algorithms? Explain your answer.\n",
    "\n",
    "* How do you think, are the hyperparameters for the decision trees you found in 2.5 optimal for trees used in ensemble? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8fc95a2b206bdae1",
     "locked": false,
     "points": 35,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-241b7691ab44cbfb",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### 2.7. Random Forest\n",
    "Now we will work with the Random Forest (its `sklearn` implementation).\n",
    "\n",
    "* * Plot `f1` and `accuracy` scores plots w.r.t. the number of trees in Random Forest.\n",
    "\n",
    "* What is the optimal number of trees you've got? Is it different from the optimal number of logistic regressions/decision trees in 2.6? Explain the results briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-888755d0f3d91620",
     "locked": false,
     "points": 15,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-99191c0852538d4d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### 2.8. Learning curve\n",
    "Your goal is to estimate, how does the model behaviour change with the increase of the `train` dataset size.\n",
    "\n",
    "* Split the training data into 10 equal (almost) parts. Then train the models from above (Logistic regression, Desicion Tree, Random Forest) with optimal hyperparameters you have selected on 1 part, 2 parts (combined, so the train size in increased by 2 times), 3 parts and so on.\n",
    "\n",
    "* Build a plot of `accuracy` and `f1` scores on `test` part, varying the `train` dataset size (so the axes will be score - dataset size.\n",
    "\n",
    "* Analyse the final plot. Can you make any conlusions using it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e39bc7e7dff61ff9",
     "locked": false,
     "points": 15,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
